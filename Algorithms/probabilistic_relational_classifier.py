import numpy as np
import networkx as nx

class ProbabilisticRelationalClassifier:

    def __init__(self, G, classes):
        self.G = G
        self.k = classes

        self.nodes = list(self.G.nodes())

        self.idx_to_label_vector = {}
        self.node_to_idx = {node : idx for idx, node in enumerate(self.nodes)}
        self.idx_to_node = {idx : node for idx, node in enumerate(self.nodes)}
        
        self.get_labeled_nodes()
        self.get_unlabelled_nodes()
    
    def get_labeled_nodes(self):
        labeled_nodes = set()
 
        for node in self.G.nodes():
            idx = self.node_to_idx[node]
            label = self.G.nodes[node].get("label", None)
            if label is not None:
                labeled_nodes.add(node)
                self.idx_to_label_vector[idx] = np.zeros(self.k, )
                self.idx_to_label_vector[idx][int(label)] = 1
        
        self.labeled_nodes = labeled_nodes

    def get_unlabelled_nodes(self):
        unlabeled_nodes = set()

        for node in self.G.nodes():
            idx = self.node_to_idx[node]
            label = self.G.nodes[node].get("label", None)
            if label is None:
                unlabeled_nodes.add(node)
                self.idx_to_label_vector[idx] = np.random.uniform(low = 0, high=1, size=(self.k))
                self.idx_to_label_vector[idx] /= np.sum(self.idx_to_label_vector[idx])
        
        self.un_labeled_nodes = unlabeled_nodes
        

    def get_neighbour_info(self, node):
        
        neighbour_labels = [] 
        neighbour_weights = [] 

        for neighbour in self.G.adj[node]:
            neighbour_idx = self.node_to_idx[neighbour]
            neighbour_label = self.idx_to_label_vector[neighbour_idx]
            neighbour_edge_weight = self.G.adj[node][neighbour].get('weight', 1.0)

            neighbour_labels.append(neighbour_label)
            neighbour_weights.append(neighbour_edge_weight)
            
        return np.array(neighbour_labels), np.array(neighbour_weights)
    
    def calculate_neighbour_aggregation(self):

        all_neighbour_info = []
        for node_idx in self.idx_to_node:
            node = self.idx_to_node[node_idx]
            neighbour_labels, neighbour_weights = self.get_neighbour_info(node)
            
            if neighbour_labels.size == 0:  # no neighbors
                agg = np.ones(self.k) / self.k
            else:
                agg = np.mean(neighbour_weights[:, None] * neighbour_labels, axis=0)
            all_neighbour_info.append(agg)

        return np.stack(all_neighbour_info, axis=0)

    def forward(self):
        neighbour_info = self.calculate_neighbour_aggregation()

        for node in self.nodes:
            node_idx = self.node_to_idx[node]
            if node in self.un_labeled_nodes:
                self.idx_to_label_vector[node_idx] = neighbour_info[node_idx]

        return

### Driver code : Generated by the AI

import numpy as np
import networkx as nx
import matplotlib.pyplot as plt

from networkx.algorithms.community import greedy_modularity_communities

# -------------------------
# Metrics helpers (no sklearn)
# -------------------------
def per_class_metrics(y_true, y_pred, n_classes):
    """
    y_true, y_pred: 1D int arrays
    returns: dict with per-class precision/recall/f1 + overall accuracy + macro averages
    """
    y_true = np.asarray(y_true, dtype=int)
    y_pred = np.asarray(y_pred, dtype=int)

    acc = float(np.mean(y_true == y_pred)) if len(y_true) else 0.0

    precisions, recalls, f1s = [], [], []
    per_class = []

    for c in range(n_classes):
        tp = int(np.sum((y_pred == c) & (y_true == c)))
        fp = int(np.sum((y_pred == c) & (y_true != c)))
        fn = int(np.sum((y_pred != c) & (y_true == c)))

        prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0
        rec  = tp / (tp + fn) if (tp + fn) > 0 else 0.0
        f1   = (2 * prec * rec) / (prec + rec) if (prec + rec) > 0 else 0.0

        precisions.append(prec); recalls.append(rec); f1s.append(f1)
        per_class.append((c, prec, rec, f1, tp, fp, fn))

    return {
        "accuracy": acc,
        "macro_precision": float(np.mean(precisions)) if n_classes else 0.0,
        "macro_recall": float(np.mean(recalls)) if n_classes else 0.0,
        "macro_f1": float(np.mean(f1s)) if n_classes else 0.0,
        "per_class": per_class,
    }

def normalize_label_vectors_inplace(prc):
    """Optional safety: ensure each vector sums to 1 (keeps things probability-like)."""
    for idx in range(len(prc.nodes)):
        v = prc.idx_to_label_vector[idx]
        s = float(np.sum(v))
        if s > 0:
            prc.idx_to_label_vector[idx] = v / s
        else:
            prc.idx_to_label_vector[idx] = np.ones(prc.k) / prc.k

# -------------------------
# Main demo using YOUR PRC
# -------------------------
def main(seed=7, label_rate=0.2, epochs=25):
    np.random.seed(seed)

    # 1) Real NetworkX dataset (graph of Les MisÃ©rables character co-occurrence)
    G = nx.les_miserables_graph()

    # 2) Create multi-class labels via community detection (pseudo ground-truth classes)
    communities = list(greedy_modularity_communities(G))
    node_to_class = {}
    for cid, comm in enumerate(communities):
        for node in comm:
            node_to_class[node] = cid

    k = len(communities)
    print(f"Graph: |V|={G.number_of_nodes()}, |E|={G.number_of_edges()}, classes(k)={k}")

    # Attach full labels first (we'll hide most next)
    for node, c in node_to_class.items():
        G.nodes[node]["label"] = int(c)

    # 3) Hide labels to simulate unknowns, but keep at least 1 labeled node per class
    nodes = list(G.nodes())
    nodes_by_class = {c: [n for n in nodes if node_to_class[n] == c] for c in range(k)}

    labeled_keep = set()
    for c in range(k):
        # ensure each class has at least one labeled seed
        labeled_keep.add(np.random.choice(nodes_by_class[c]))

    target_labeled = max(len(labeled_keep), int(round(label_rate * len(nodes))))
    remaining = [n for n in nodes if n not in labeled_keep]
    if target_labeled > len(labeled_keep):
        extra = np.random.choice(remaining, size=(target_labeled - len(labeled_keep)), replace=False)
        labeled_keep.update(extra.tolist())

    for n in nodes:
        if n not in labeled_keep:
            G.nodes[n]["label"] = None  # your code treats None as unlabeled

    # 4) Initialize PRC (your __init__ calls get_labeled_nodes + get_unlabelled_nodes)
    prc = ProbabilisticRelationalClassifier(G, classes=k)

    # Prepare split for evaluation
    unlabeled_nodes = [n for n in nodes if n not in labeled_keep]

    def predict_all():
        y_true_all, y_pred_all = [], []
        y_true_u, y_pred_u = [], []

        for n in nodes:
            idx = prc.node_to_idx[n]
            pred = int(np.argmax(prc.idx_to_label_vector[idx]))
            true = int(node_to_class[n])

            y_true_all.append(true); y_pred_all.append(pred)
            if n in unlabeled_nodes:
                y_true_u.append(true); y_pred_u.append(pred)

        return (y_true_all, y_pred_all), (y_true_u, y_pred_u)

    history = {"acc_unlabeled": [], "macro_f1_unlabeled": []}

    # 5) Training loop = repeated forward passes
    for ep in range(1, epochs + 1):
        prc.forward()
        normalize_label_vectors_inplace(prc)  # optional but nice

        (yt_all, yp_all), (yt_u, yp_u) = predict_all()

        m_all = per_class_metrics(yt_all, yp_all, k)
        m_u   = per_class_metrics(yt_u,   yp_u,   k)

        history["acc_unlabeled"].append(m_u["accuracy"])
        history["macro_f1_unlabeled"].append(m_u["macro_f1"])

        # Print per forward (epoch)
        print(f"\nEpoch {ep:02d}")
        print(f"  Unlabeled: acc={m_u['accuracy']:.4f}  macroP={m_u['macro_precision']:.4f}  macroR={m_u['macro_recall']:.4f}  macroF1={m_u['macro_f1']:.4f}")
        print(f"  Overall  : acc={m_all['accuracy']:.4f}  macroP={m_all['macro_precision']:.4f}  macroR={m_all['macro_recall']:.4f}  macroF1={m_all['macro_f1']:.4f}")

        # Per-class (on unlabeled only, since that's the real task)
        for (c, prec, rec, f1, tp, fp, fn) in m_u["per_class"]:
            print(f"    class {c:02d}: P={prec:.3f} R={rec:.3f} F1={f1:.3f} (TP={tp}, FP={fp}, FN={fn})")

    # 6) Plot after training
    xs = np.arange(1, epochs + 1)
    plt.figure()
    plt.plot(xs, history["acc_unlabeled"], label="Unlabeled Accuracy")
    plt.plot(xs, history["macro_f1_unlabeled"], label="Unlabeled Macro-F1")
    plt.xlabel("Epoch")
    plt.ylabel("Score")
    plt.title("PRC performance over forward iterations")
    plt.legend()
    plt.show()


if __name__ == "__main__":
    main(seed=7, label_rate=0.2, epochs=25)
